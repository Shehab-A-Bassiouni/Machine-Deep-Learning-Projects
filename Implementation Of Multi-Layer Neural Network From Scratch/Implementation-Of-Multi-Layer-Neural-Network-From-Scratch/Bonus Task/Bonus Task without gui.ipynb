{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d445550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class task_3():\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        df_train=pd.read_csv(\"mnist_train.csv\")\n",
    "        df_test = pd.read_csv(\"mnist_test.csv\")\n",
    "        \n",
    "        X_train= df_train.iloc[:, 1:]\n",
    "        \n",
    "        Y_train=df_train.iloc[:, 0]\n",
    "        \n",
    "        X_test =df_test.iloc[:, 1:]\n",
    "        \n",
    "        Y_test = df_test.iloc[:, 0]\n",
    "        \n",
    "        X_train=X_train.to_numpy()\n",
    "        X_test=X_test.to_numpy()\n",
    "        Y_train = Y_train.to_numpy()\n",
    "        Y_test = Y_test.to_numpy()\n",
    "        \n",
    "        \n",
    "        X_train=X_train.T\n",
    "        X_test=X_test.T\n",
    "        \n",
    "        \n",
    "        Y_train = np.resize(Y_train , (Y_train.shape[0],1))\n",
    "        ecoding_train_y = np.zeros((Y_train.shape[0], 10))\n",
    "        ecoding_train_y[np.arange(Y_train.shape[0]), Y_train.flatten()] = 1\n",
    "        Y_train = ecoding_train_y.astype(int)\n",
    "        Y_train=Y_train.T\n",
    "        \n",
    "        \n",
    "        Y_test = np.resize(Y_test , (Y_test.shape[0],1))\n",
    "        ecoding_test_y = np.zeros((Y_test.shape[0], 10))\n",
    "        ecoding_test_y[np.arange(Y_test.shape[0]), Y_test.flatten()] = 1\n",
    "        Y_test = ecoding_test_y.astype(int)\n",
    "        Y_test =Y_test.T\n",
    "        \n",
    "        \n",
    "        \n",
    "        return X_train ,Y_train ,X_test,Y_test\n",
    "        \n",
    "    \n",
    "    \n",
    "    def activation(self,x,actv):\n",
    "        if actv == \"sigmoid\":\n",
    "            return expit(x)\n",
    "\n",
    "        elif actv==\"tanh\":\n",
    "            return np.tanh(x)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def prepare_layers(self ,hidden_layers , X_train ):\n",
    "        np.random.seed(1)\n",
    "        layers_dimentions = []\n",
    "        layers_dimentions.append(X_train.shape[0])\n",
    "        for i in range(0,len(hidden_layers)):\n",
    "             layers_dimentions.append(hidden_layers[i])\n",
    "        layers_dimentions.append(10)\n",
    "        parameters={}\n",
    "        for i in range(1,len(layers_dimentions)):\n",
    "            parameters['W' + str(i)] = np.random.randn(layers_dimentions[i],layers_dimentions[i-1] ) *0.01\n",
    "            parameters['b' + str(i)] = np.zeros((layers_dimentions[i],1))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def forward(self ,X_train,parameters,activ,bias_or_not):\n",
    "        A = X_train\n",
    "        L=len(parameters) // 2\n",
    "        caching={'A0':A}\n",
    "        for i in range(1,L+1):\n",
    "            W = parameters['W' + str(i)]\n",
    "            b = parameters['b' + str(i)]\n",
    "            if bias_or_not == True:\n",
    "               \n",
    "                \n",
    "                z=np.dot(W,A)+b\n",
    "            else:\n",
    "                z=np.dot(W,A)\n",
    "                \n",
    "            caching['Z' + str(i)]=z\n",
    "            if activ == \"sigmoid\":\n",
    "                A= self.activation(z,\"sigmoid\")\n",
    "            else:\n",
    "                A= self.activation(z,\"tanh\")\n",
    "                \n",
    "            caching['A' + str(i)]=A\n",
    "        \n",
    "        return caching ,A\n",
    "    \n",
    "\n",
    "        \n",
    "    def transform_output(self,Y):\n",
    "        modified_lists = []\n",
    "        for lst in Y:\n",
    "            max_prob = max(lst)\n",
    "            modified_lst = [1 if prob == max_prob else 0 for prob in lst]\n",
    "            modified_lists.append(modified_lst)\n",
    "        modified_lists = np.array(modified_lists)\n",
    "        return modified_lists\n",
    "\n",
    "    def backward(self, parameters,Y_train ,caching ):\n",
    "        gradiants={}\n",
    "        L = len(parameters)//2\n",
    "       \n",
    "        dZ=caching[\"A\"+str(L)] - Y_train\n",
    "        gradiants['dW' + str(L)] = np.dot(dZ,caching['A' + str(L-1)].T) / Y_train.shape[1]\n",
    "        gradiants['db' + str(L)] = np.sum(dZ, axis=1, keepdims=True) / Y_train.shape[1]\n",
    "        \n",
    "        for i in range(L-1,0,-1):\n",
    "            dA = np.dot( parameters['W' + str(i+1)].T,dZ)\n",
    "            dZ = dA * caching['A' + str(i)] * (1 - caching['A' + str(i)])\n",
    "            gradiants['dW' + str(i)] = np.dot(dZ,caching['A' + str(i-1)].T ) / Y_train.shape[1]\n",
    "            gradiants['dW' + str(i)] += (0.001 / Y_train.shape[1])*gradiants['dW' + str(i)]\n",
    "            gradiants['db' + str(i)] = np.sum(dZ, axis=1, keepdims=True) / Y_train.shape[1]\n",
    "        return gradiants\n",
    "         \n",
    "    def update(self,parameters, gradiants, learn_rate):\n",
    "        L = len(parameters) // 2\n",
    "        for i in range(1, L+1):\n",
    "            parameters['W' + str(i)] -= learn_rate * gradiants['dW' + str(i)]\n",
    "            parameters['b' + str(i)] -= learn_rate * gradiants['db' + str(i)]\n",
    "        return parameters\n",
    "    \n",
    "    def train(self ,X_train,Y_train,X_test,Y_test,activ,bias_or_not,learn_rate,epoches,hidden_layers):\n",
    "        parameters = self.prepare_layers(hidden_layers,X_train)\n",
    "        max_accuracy=0\n",
    "        values=parameters\n",
    "        for i in range(0,epoches):\n",
    "            caching,_=self.forward(X_train,parameters,activ,bias_or_not)\n",
    "            gradiants=self.backward(parameters,Y_train,caching)\n",
    "            parameters=self.update(parameters, gradiants, learn_rate)\n",
    "            result=self.test(X_test,parameters,activ,bias_or_not)\n",
    "            acc = self.accuracy(result,Y_test)\n",
    "            if acc[4] > max_accuracy:\n",
    "                max_accuracy=acc[4]\n",
    "                values=parameters\n",
    "        print(f\"final result ----- highest accuracy is {max_accuracy}\")\n",
    "        return values\n",
    "    \n",
    "    def test(self,X_test,parameters,activ,bias_or_not):\n",
    "        caching,A = self.forward(X_test,parameters,activ,bias_or_not)\n",
    "        \n",
    "        result=self.transform_output(A.T)\n",
    "        return result\n",
    "    \n",
    "    def accuracy(self,result,labels):\n",
    "        #for class 0\n",
    "        tp=0\n",
    "        fp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        labels=self.transform_output(labels.T)\n",
    "        for i in range(0,labels.shape[0]):\n",
    "            if np.argmax(result[i]) == np.argmax(labels[i]):\n",
    "                tp+=1\n",
    "            elif np.argmax(result[i]) == 0 and np.argmax(labels[i])!=0:\n",
    "                fp+=1\n",
    "            \n",
    "            elif np.argmax(result[i]) != 0 and np.argmax(labels[i])!=0:\n",
    "                tn+=1     \n",
    "                \n",
    "            elif np.argmax(result[i]) != 0 and np.argmax(labels[i])==0:\n",
    "                fn+=1\n",
    "                \n",
    "        accuracy = (tp / labels.shape[0] ) * 100\n",
    "        \n",
    "        return tp,fp,tn,fn,accuracy\n",
    "    \n",
    "    def sto_train(self,X_train,Y_train,X_test,Y_test,activ,bias_or_not,learn_rate,epoches,hidden_layers):\n",
    "        \n",
    "        max_accuracy = 0\n",
    "        parameters = self.prepare_layers(hidden_layers,X_train)\n",
    "        values =parameters\n",
    "        counter = 100 \n",
    "        for c in range(0 , int(X_train.shape[1]/100)):\n",
    "            tmp_x = X_train[:,:counter]\n",
    "            tmp_y =Y_train[:,:counter]\n",
    "            for i in range(0,epoches):\n",
    "                caching,_=self.forward(tmp_x,parameters,activ,bias_or_not)\n",
    "                gradiants=self.backward(parameters,tmp_y,caching)\n",
    "                parameters=self.update(parameters, gradiants, learn_rate)\n",
    "            result=self.test(X_test,parameters,activ,bias_or_not)\n",
    "            acc = self.accuracy(result,Y_test)\n",
    "            print(f\"stochastic: For Round {c}, Accuracy is {acc[4]} , tp {acc[0]}, fp {acc[1]}, tn {acc[2]}, fn {acc[3]}\")\n",
    "            counter+=100\n",
    "            if acc[4] > max_accuracy:\n",
    "                max_accuracy=acc[4]\n",
    "                values=parameters\n",
    "        print(f\"final result ----- highest accuracy is {max_accuracy}\")\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_way = \"stochastic\"\n",
    "#learn_way = \"full batch\"\n",
    "activ = \"tanh\"\n",
    "bias_or_not = True\n",
    "learn_rate=0.1\n",
    "epoches=1\n",
    "hidden_layers = [5000,4000,3000,1000]\n",
    "obj = task_3()\n",
    "X_train ,Y_train ,X_test,Y_test = obj.preprocess_data()\n",
    "\n",
    "if learn_way == \"stochastic\":\n",
    "    parameters=obj.sto_train(X_train,Y_train,X_test,Y_test,activ,bias_or_not,learn_rate,epoches,hidden_layers)\n",
    "elif learn_way ==\"full batch\":\n",
    "    parameters=obj.train(X_train,Y_train,X_test,Y_test,activ,bias_or_not,learn_rate,epoches,hidden_layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ad11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=obj.test(X_test,parameters,activ,bias_or_not)\n",
    "tp,fp,tn,fn,accuracy = obj.accuracy(result,Y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcfbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result (tanh)| stochastic | 5 epoches | learn rate 0.1 | layers [5000,4000,3000,1000] | bias\n",
    "#max_accuracy =75 |    tp= 7405     |     fp= 331   |    tn= 2236      |         fn=38\n",
    "\n",
    "# result (tanh)| stochastic | 5 epoches | learn rate 0.1 | layers [5000,4000,3000,1000] | no bias\n",
    "#max_accuracy = 76.5|    tp= 7636     |     tn= 2039   |    fp=  285    |         fn=40\n",
    "\n",
    "# result (sigmoid)| stochastic | 5 epoches | learn rate 0.1 | layers [5000,4000,3000,1000] | bias\n",
    "#max_accuracy =11.5 |    tp= 1135     |     tn= 7885   |    fp= 0     |         fn= 980\n",
    "# result (sigmoid)| stochastic | 5 epoches | learn rate 0.1 | layers [5000,4000,3000,1000] | no bias\n",
    "#max_accuracy =11.5 |    tp=  1135    |     tn=  7885  |    fp=  0    |         fn=980"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aho)",
   "language": "python",
   "name": "aho"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
